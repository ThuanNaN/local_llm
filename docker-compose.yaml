services:
    ollama:
        container_name: ollama
        image: ollama/ollama:latest
        restart: unless-stopped
        ports:
            - "11434:11434"
        volumes:
            - ollama:/root/.ollama
        pull_policy: always
        tty: true
        deploy:
            resources:
                reservations:
                    devices:
                        - driver: nvidia
                          count: 1
                          capabilities: [gpu]


    openui:
        container_name: openui
        image: ghcr.io/wandb/openui
        restart: unless-stopped
        depends_on:
            - ollama
        ports:
            - "3002:7878"
        environment:
            - OLLAMA_HOST=http://ollama:11434

    open-webui:
        container_name: open-webui
        image: ghcr.io/open-webui/open-webui:main
        restart: unless-stopped
        volumes:
            - open-webui:/app/backend/data
        depends_on:
            - ollama
        ports:
            - "3001:8080"
        environment:
            - OLLAMA_BASE_URL=http://ollama:11434
        extra_hosts:
            - host.docker.internal:host-gateway
            
volumes:
    ollama:
    open-webui:
